from spark_session_builder import build
from fetch_tweet_data import scrape_tweet_data
from tweet_cleaner import preprocess_tweet
from tweet_feature_extractor import extract_features
from text_labeler import label_data
from model_trainer import train
from model_predictor import test
from stream_predictor import stream_predict
from tweet_processor import build_filtered_json_list, build_tweet_search_cursor
from tweet_stream_producer import push_to_kafka
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from df_persist import write_df, read_df, write_pipeline
from pyspark.ml import PipelineModel
from functools import partial
import argparse
from time import sleep
from ml_boms import (FetchDataModel, Operations, FeatureColumns, TaskBaseModel,
                     TrainModel, StreamProduceModel, DataTablePathConfig,
                     PipelinePathConfig)


def build_arg_parser():
    parser = argparse.ArgumentParser()

    parser.add_argument("--operation",
                        help="perform specified operation",
                        choices=[i.name for i in Operations],
                        required=True)

    parser.add_argument("--params",
                        help="Params for the operation",
                        required=True)

    parser.add_argument("--enable_tweet_scraping",
                        help="Enable Tweet Scraping",
                        action="store_true")

    return parser


def run_fetch():

    fetch_model = FetchDataModel.parse_raw(args.params)

    date_to_fetch = fetch_model.fetch_date.strftime('%Y-%m-%d') or '2021-12-25'

    total_tweets_to_fetch = fetch_model.total_tweets_to_fetch or 120

    saver_func = partial(write_df,
                         save_path=data_table_paths.hydrated_tweet_table_path,
                         save_mode="append") if fetch_model.save else None
    scrape_tweet_data(date_to_fetch, spark, saver_func, total_tweets_to_fetch)

    if fetch_model.verbose:
        read_df(spark, data_table_paths.hydrated_tweet_table_path).show()


def run_clean():

    clean_model = TaskBaseModel.parse_raw(args.params)

    pipeline_model, preprocessed_tweet_df = preprocess_tweet(
        read_df(spark, data_table_paths.hydrated_tweet_table_path))
    if clean_model.save:
        write_df(preprocessed_tweet_df,
                 data_table_paths.cleaned_tweet_table_path)
        write_pipeline(pipeline_model, pipeline_paths.tweet_cleaner_pipeline)

    if clean_model.verbose:
        read_df(spark, data_table_paths.cleaned_tweet_table_path).select(
            'text', 'lemmatized_text').show()
        print(PipelineModel.load(pipeline_paths.tweet_cleaner_pipeline).stages)


def run_feature_extract():

    feature_model = TaskBaseModel.parse_raw(args.params)

    cleaned_df = read_df(spark, data_table_paths.cleaned_tweet_table_path)

    feature_pipline_model, feature_df = extract_features(cleaned_df)

    if feature_model.save:
        write_df(feature_df, data_table_paths.feature_tweet_table_path)
        write_pipeline(feature_pipline_model,
                       pipeline_paths.tweet_feature_pipeline)

    if feature_model.verbose:
        print(f'Total available records {cleaned_df.count()}')
        read_df(spark, data_table_paths.feature_tweet_table_path).select(
            FeatureColumns.lemmatized_text.name,
            FeatureColumns.word2_vec_features.name).show()

        print(feature_df.columns)
        print(PipelineModel.load(pipeline_paths.tweet_feature_pipeline).stages)


def run_labeler():

    label_model = TaskBaseModel.parse_raw(args.params)

    featured_df = read_df(spark, data_table_paths.feature_tweet_table_path)

    labeled_df = label_data(featured_df)

    if label_model.save:
        write_df(labeled_df, data_table_paths.labeled_tweet_table_path)

    if label_model.verbose:
        read_df(spark, data_table_paths.labeled_tweet_table_path).select(
            'text', 'train_sentiment', 'train_sentiment_index').show()


def run_train():

    train_model = TrainModel.parse_raw(args.params)

    tweet_df = read_df(spark, data_table_paths.labeled_tweet_table_path)

    model, predictions = train(tweet_df, train_model)

    if train_model.save:
        write_pipeline(model, pipeline_paths.trained_model_pipeline)

    if train_model.verbose:
        evaluator = MulticlassClassificationEvaluator(
            labelCol='train_sentiment_index',
            predictionCol=model.stages[-1].getOrDefault('predictionCol'),
            metricName='accuracy')

        print(evaluator.evaluate(predictions))


def run_test():

    test_model = TaskBaseModel.parse_raw(args.params)

    print(f'Test Model : {test_model.dict()}')

    predictions = test(spark)
    if test_model.verbose:
        predictions.select(['text', 'predicted_sentiment_string']).show()


def run_producer():

    producer_model = StreamProduceModel.parse_raw(args.params)

    if producer_model.enable_tweet_scraping:
        while True:
            search_words = '''COVID19 OR COVID-19 OR 
            coronavirus OR coronavaccine OR 
            coronaoutbreak OR omicron OR booster -filter:retweet'''

            date_since = "2021-12-31"
            tweets_cursor = build_tweet_search_cursor(search_words, date_since)
            tweet_list = build_filtered_json_list(tweets_cursor.items(25),
                                                  extract_user_details=True)
            if tweet_list:
                push_to_kafka(tweet_list)
            else:
                print('No tweets fetched')
            sleep(30)
    else:
        print("Log Scraping Disabled")


def run_stream_predictor():
    task_model = TaskBaseModel.parse_raw(args.params)
    stream_predict(spark, task_model, data_table_paths.predicted_tweets_path)


if __name__ == "__main__":
    parser = build_arg_parser()
    args = parser.parse_args()

    data_table_paths = DataTablePathConfig.get_data_table_path_config()
    pipeline_paths = PipelinePathConfig.get_pipeline_path_config()

    if args.operation:
        spark = build()
        op = args.operation.lower()

        if op == "fetch":
            run_fetch()
        if op == "clean":
            run_clean()
        if op == "feature_extract":
            run_feature_extract()
        if op == "label":
            run_labeler()
        if op == "model_train":
            run_train()
        if op == "model_test":
            run_test()
        if op == "produce_tweet":
            run_producer()
        if op == "stream_predict":
            run_stream_predictor()

    print("Thanks for visiting")
