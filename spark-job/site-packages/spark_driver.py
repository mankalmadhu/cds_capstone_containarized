from dotenv import load_dotenv

load_dotenv()

from spark_session_builder import build
from fetch_tweet_data import scrape_tweet_data
from tweet_cleaner import preprocess_tweet
from tweet_feature_extractor import extract_features
from text_labeler import label_data
from model_trainer import train
from model_predictor import test
from stream_predictor import stream_predict
from tweet_processor import build_filtered_json_list, build_tweet_search_cursor
from tweet_stream_producer import push_to_kafka
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from df_persist import write_df, read_df, write_pipeline
from pyspark.ml import PipelineModel
from functools import partial
import argparse
import os
from time import sleep
import datetime
from operations import Operations


def build_arg_parser():
    parser = argparse.ArgumentParser()

    parser.add_argument("--operation",
                        help="perform specified operation",
                        choices=[i.name for i in Operations])

    parser.add_argument("-s",
                        "--save",
                        help="Save the dataframe",
                        action="store_true")

    parser.add_argument("-v",
                        "--verbose",
                        help="Print Debug Output",
                        action="store_true")

    parser.add_argument("--fetch_date",
                        help="Tweet Date to fetch YYYY-MM-DD",
                        type=lambda s: datetime.date.strptime(s, '%Y-%m-%d'))

    parser.add_argument("--total_tweets_to_fetch",
                        type=int,
                        choices=range(0, 10000))

    return parser


def run_fetch():
    date_to_fetch = args.fetch_date.strftime('%Y-%m-%d') or '2021-12-25'

    total_tweets_to_fetch = args.total_tweets_to_fetch or 120

    saver_func = partial(write_df,
                         save_path=hydrated_table_path,
                         save_mode="append") if args.save else None
    scrape_tweet_data(date_to_fetch, spark, saver_func, total_tweets_to_fetch)

    if args.verbose:
        read_df(spark, hydrated_table_path).show()


def run_clean():
    cleaner_pipeline_path = os.environ['tweet_cleaner_pipeline']
    pipeline_model, preprocessed_tweet_df = preprocess_tweet(
        read_df(spark, hydrated_table_path))
    if args.save:
        write_df(preprocessed_tweet_df, clean_table_path)
        write_pipeline(pipeline_model, cleaner_pipeline_path)

    if args.verbose:
        read_df(spark, clean_table_path).select('text',
                                                'lemmatized_text').show()
        print(PipelineModel.load(cleaner_pipeline_path).stages)


def run_feature_extract():
    feature_pipeline_path = os.environ['tweet_feature_pipeline']

    cleaned_df = read_df(spark, clean_table_path)
    print(f'Total available records {cleaned_df.count()}')

    feature_pipline_model, feature_df = extract_features(cleaned_df)

    if args.save:
        write_df(feature_df, feature_table_path)
        write_pipeline(feature_pipline_model, feature_pipeline_path)

    if args.verbose:
        read_df(spark, feature_table_path).select('lemmatized_text',
                                                  'assembled_features').show()
        print(PipelineModel.load(feature_pipeline_path).stages)


def run_labeler():

    featured_df = read_df(spark, feature_table_path)

    labeled_df = label_data(featured_df)

    if args.save:
        write_df(labeled_df, labeled_table_path)

    if args.verbose:
        read_df(spark,
                labeled_table_path).select('text', 'text_blob_sentiment',
                                           'text_blob_sentiment_index').show()


def run_train():

    tweet_df = read_df(spark, labeled_table_path)

    model, predictions = train(tweet_df)
    trained_model_path = os.environ['trained_model_pipeline']

    if args.save:
        write_pipeline(model, trained_model_path)

    if args.verbose:
        evaluator = MulticlassClassificationEvaluator(
            labelCol='text_blob_sentiment_index',
            predictionCol='prediction',
            metricName='accuracy')

        print(evaluator.evaluate(predictions))


def run_test():

    predictions = test(spark)
    if args.verbose:
        predictions.select(['text', 'predicted_sentiment_string']).show()


def run_producer():

    while True:
        search_words = '''COVID19 OR COVID-19 OR 
        coronavirus OR coronavaccine OR 
        coronaoutbreak OR omicron OR booster -filter:retweet'''

        date_since = "2021-12-31"
        tweets_cursor = build_tweet_search_cursor(search_words, date_since)
        tweet_list = build_filtered_json_list(tweets_cursor.items(25),
                                              extract_user_details=True)
        if tweet_list:
            push_to_kafka(tweet_list)
        else:
            print('No tweets fetched')
        sleep(30)


def run_stream_predictor():
    stream_predict(spark)


if __name__ == "__main__":
    parser = build_arg_parser()
    args = parser.parse_args()

    hydrated_table_path = os.environ['hydrated_tweet_table_path']
    clean_table_path = os.environ['cleaned_tweet_table_path']
    feature_table_path = os.environ['feature_tweet_table_path']
    labeled_table_path = os.environ['labeled_tweet_table_path']

    if args.operation:
        spark = build()
        op = args.operation.lower()

        if op == "fetch":
            run_fetch()
        if op == "clean":
            run_clean()
        if op == "feature_extract":
            run_feature_extract()
        if op == "label":
            run_labeler()
        if op == "model_train":
            run_train()
        if op == "model_test":
            run_test()
        if op == "produce_tweet":
            run_producer()
        if op == "stream_predict":
            run_stream_predictor()

    print("Thanks for visiting")
