import pyspark.sql.functions as fn
from pyspark.sql.types import StringType, StructType, ArrayType, StructField
from pyspark.sql.functions import from_json, col, explode
from functools import partial
from df_persist import write_df
from ml_boms import DataTablePathConfig, KafkaCreds

from model_predictor import predict, load_pipelines

schema = ArrayType(\
  StructType([\
              StructField("id_str", StringType()),
              StructField("text", StringType()),
              StructField("user_location", StringType()),
              StructField("user_name", StringType()),
              StructField("user_followers", StringType()),
              StructField("tweet_created_at", StringType()),
              StructField("source", StringType()),
              StructField("retweet_count", StringType()),
              StructField("favorite_count", StringType()),
             ]))


def build_spark_stream_df(spark):
    kafka_creds = KafkaCreds.get_kafka_creds()

    return spark.readStream.format("kafka")\
        .option("kafka.bootstrap.servers", kafka_creds.confluent_bootstrap_server)\
        .option("kafka.security.protocol", "SASL_SSL")\
        .option("kafka.sasl.jaas.config", "org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';".format(kafka_creds.confluent_api_key, kafka_creds.confluent_secret)) \
        .option("kafka.ssl.endpoint.identification.algorithm","https")\
        .option("kafka.sasl.mechanism", "PLAIN")\
        .option("subscribe", kafka_creds.confluent_streaming_topic)\
        .option("startingOffsets", "earliest")\
        .option("failOnDataLoss", "false")\
        .load()\
        .withColumn('key',fn.col("key").cast(StringType()))\
        .withColumn('value',fn.col("value").cast(StringType()))\
        .select('topic', 'partition', 'offset','timestamp', 'timestampType', 'key','value')


def predict_sentiment_on_tweets(data_frame, epoch_id, **kwargs):
    predicted_tweets_path = kwargs['predicted_tweets_path']
    pipelines_tuple = kwargs['pipelines_tuple']
    task_model = kwargs['task_model']

    tweet_stream_df_data = data_frame.select(
        explode(from_json(data_frame.value,
                          schema)).alias("data")).select('data.*')

    if task_model.verbose:
        print("Before Prediction")
        tweet_stream_df_data.show()

    tweet_stream_df_predictions = predict(pipelines_tuple,
                                          tweet_stream_df_data)

    if task_model.verbose:
        print("After Prediction")
        tweet_stream_df_predictions.show()

    if task_model.save:
        write_df(tweet_stream_df_predictions,
                 predicted_tweets_path,
                 save_mode="append")


def stream_predict(spark, task_model, predicted_tweets_path):

    predictor_func = partial(predict_sentiment_on_tweets,
                             pipelines_tuple=load_pipelines(),
                             predicted_tweets_path=predicted_tweets_path,
                             task_model=task_model)

    tweet_stream_df = build_spark_stream_df(spark)
    tweet_stream_df.writeStream \
    .foreachBatch(predictor_func) \
    .queryName("tweet_sentiment_predictor") \
    .start()

    while True:
        pass
