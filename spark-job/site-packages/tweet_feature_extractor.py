from pyspark.ml.feature import CountVectorizer
from pyspark.ml.feature import HashingTF, IDF
from pyspark.sql.functions import col, concat_ws
from pyspark.ml.feature import NGram
from pyspark.ml.feature import Word2Vec
from pyspark.ml import Pipeline

from ml_boms import FeatureColumns


def extract_features(cleaned_tweet_df):

    hashing = HashingTF(inputCol=FeatureColumns.lemmatized_text.name,
                        outputCol="raw_token")
    tf_idf = IDF(inputCol="raw_token",
                 outputCol=FeatureColumns.idf_features.name)

    count_vectorizer = CountVectorizer(
        inputCol=FeatureColumns.lemmatized_text.name,
        outputCol=FeatureColumns.cv_features.name,
        minDF=1.0)

    bi_gram = NGram(n=2,
                    inputCol=FeatureColumns.lemmatized_text.name,
                    outputCol="bi_grams")
    hashing_bi_gram = HashingTF(inputCol="bi_grams",
                                outputCol="hashing_bi_gram_raw_token")

    tf_idf_bigram = IDF(inputCol="hashing_bi_gram_raw_token",
                        outputCol=FeatureColumns.idf_bi_gram_features.name)

    count_vectorizer_bigram = CountVectorizer(
        inputCol="bi_grams",
        outputCol=FeatureColumns.cv_bi_gram_features.name,
        minDF=1.0)

    word2_vectorizer = Word2Vec(
        vectorSize=50,
        minCount=0,
        inputCol=FeatureColumns.lemmatized_text.name,
        outputCol=FeatureColumns.word2_vec_features.name)

    tweet_feature_exraction_pipeline = Pipeline(stages=[
        hashing, tf_idf, count_vectorizer, word2_vectorizer, bi_gram,
        hashing_bi_gram, tf_idf_bigram, count_vectorizer_bigram
    ])

    tweet_feature_exraction_model = tweet_feature_exraction_pipeline.fit(
        cleaned_tweet_df)

    tweet_feature_extracted_df = tweet_feature_exraction_model.transform(
        cleaned_tweet_df)

    return tweet_feature_exraction_model, tweet_feature_extracted_df
