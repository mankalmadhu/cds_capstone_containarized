import pyspark.sql.functions as fn
from pyspark.sql.types import StringType, StructType, ArrayType, StructField
from pyspark.sql.functions import from_json, col, explode
import os

from model_predictor import predict
from pyspark.ml.classification import LogisticRegressionModel

schema = ArrayType(\
  StructType([\
              StructField("id_str", StringType()),
              StructField("text", StringType()),
              StructField("user_location", StringType()),
              StructField("user_name", StringType()),
              StructField("user_followers", StringType()),
              StructField("tweet_created_at", StringType()),
              StructField("source", StringType()),
              StructField("retweet_count", StringType()),
              StructField("favorite_count", StringType()),
             ]))


def build_spark_stream_df(spark):
    confluent_topic_name = 'senti_an'
    confluent_bootstrap_servers = os.environ['confluent_bootstrap_server']
    confluent_api_key = os.environ['confluent_api_key']
    confluent_secret = os.environ['confluent_secret']


    return spark.readStream.format("kafka")\
        .option("kafka.bootstrap.servers", confluent_bootstrap_servers)\
        .option("kafka.security.protocol", "SASL_SSL")\
        .option("kafka.sasl.jaas.config", "org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';".format(confluent_api_key, confluent_secret)) \
        .option("kafka.ssl.endpoint.identification.algorithm","https")\
        .option("kafka.sasl.mechanism", "PLAIN")\
        .option("subscribe", confluent_topic_name)\
        .option("startingOffsets", "earliest")\
        .option("failOnDataLoss", "false")\
        .load()\
        .withColumn('key',fn.col("key").cast(StringType()))\
        .withColumn('value',fn.col("value").cast(StringType()))\
        .select('topic', 'partition', 'offset','timestamp', 'timestampType', 'key','value')


def predict_sentiment_on_tweets(data_frame, epoch_id):
    predicted_tweets_path = os.environ['predicted_tweets_path']
    tweet_stream_df_data = data_frame.select(
        explode(from_json(data_frame.value,
                          schema)).alias("data")).select('data.*')

    tweet_stream_df_data.show()
    tweet_stream_df_predictions = predict(LogisticRegressionModel.load,
                                          os.environ['trained_model_pipeline'],
                                          tweet_stream_df_data)

    tweet_stream_df_predictions.show()
    tweet_stream_df_predictions.write \
        .format("parquet") \
        .mode("append") \
        .save(predicted_tweets_path)


def run_stream(spark):
    tweet_stream_df = build_spark_stream_df(spark)
    tweet_stream_df.writeStream \
    .foreachBatch(predict_sentiment_on_tweets) \
    .queryName("tweet_sentiment_predictor") \
    .start()

    while True:
        pass
