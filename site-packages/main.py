from dotenv import load_dotenv

load_dotenv()

from spark_session_builder import build
from fetch_tweet_data import scrape_tweet_data
from tweet_cleaner import preprocess_tweet
from tweet_feature_extractor import extract_features
from text_labeler import label_data
from model_trainer import train
from model_test import test
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from df_persist import write_df, read_df, write_pipeline
from pyspark.ml import PipelineModel
from functools import partial
import argparse
import os


def build_arg_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("--ops",
                        help="perform specified operation",
                        choices=[
                            'fetch', 'clean', 'feature_extract', 'label',
                            'model_train', 'model_test'
                        ])
    parser.add_argument("-s",
                        "--save",
                        help="Save the dataframe",
                        action="store_true")

    parser.add_argument("-v",
                        "--verbose",
                        help="Print Debug Output",
                        action="store_true")

    return parser


def run_fetch():
    date_to_fetch = '2021-12-21'

    saver_func = partial(write_df,
                         save_path=hydrated_table_path) if args.save else None
    scrape_tweet_data(date_to_fetch,
                      spark,
                      saver_func,
                      total_tweets_to_fetch=25000)

    if args.verbose:
        read_df(spark, hydrated_table_path).show()


def run_clean():
    cleaner_pipeline_path = os.environ['tweet_cleaner_pipeline']
    pipeline_model, preprocessed_tweet_df = preprocess_tweet(
        read_df(spark, hydrated_table_path))
    if args.save:
        write_df(preprocessed_tweet_df, clean_table_path)
        write_pipeline(pipeline_model, cleaner_pipeline_path)

    if args.verbose:
        read_df(spark, clean_table_path).select('text',
                                                'lemmatized_text').show()
        print(PipelineModel.load(cleaner_pipeline_path).stages)


def run_feature_extract():
    feature_pipeline_path = os.environ['tweet_feature_pipeline']

    cleaned_df = read_df(spark, clean_table_path)

    feature_pipline_model, feature_df = extract_features(cleaned_df)

    if args.save:
        write_df(feature_df, feature_table_path)
        write_pipeline(feature_pipline_model, feature_pipeline_path)

    if args.verbose:
        read_df(spark, feature_table_path).select('lemmatized_text',
                                                  'assembled_features').show()
        print(PipelineModel.load(feature_pipeline_path).stages)


def run_labeler():

    featured_df = read_df(spark, feature_table_path)

    labeled_df = label_data(featured_df)

    if args.save:
        write_df(labeled_df, labeled_table_path)

    if args.verbose:
        read_df(spark,
                labeled_table_path).select('text', 'text_blob_sentiment',
                                           'text_blob_sentiment_index').show()


def run_train():

    tweet_df = read_df(spark, labeled_table_path)

    model, predictions = train(tweet_df)
    trained_model_path = os.environ['trained_model_pipeline']

    if args.save:
        write_pipeline(model, trained_model_path)

    if args.verbose:
        evaluator = MulticlassClassificationEvaluator(
            labelCol='text_blob_sentiment_index',
            predictionCol='prediction',
            metricName='accuracy')

        print(evaluator.evaluate(predictions))


def run_test():

    predictions = test(spark)
    if args.verbose:
        predictions.select(['text', 'predicted_sentiment_string']).show()


if __name__ == "__main__":
    parser = build_arg_parser()
    args = parser.parse_args()

    hydrated_table_path = os.environ['hydrated_tweet_table_path']
    clean_table_path = os.environ['cleaned_tweet_table_path']
    feature_table_path = os.environ['feature_tweet_table_path']
    labeled_table_path = os.environ['labeled_tweet_table_path']

    if args.ops:
        spark = build()
        op = args.ops.lower()

        if op == "fetch":
            run_fetch()
        if op == "clean":
            run_clean()
        if op == "feature_extract":
            run_feature_extract()
        if op == "label":
            run_labeler()
        if op == "model_train":
            run_train()
        if op == "model_test":
            run_test()

    print("Thanks for visiting")
