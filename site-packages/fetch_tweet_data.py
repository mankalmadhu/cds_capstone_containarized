import wget
import os
import pandas as pd
from time import sleep
import tweepy
from tweet_processor import build_filtered_json_list, build_twitter_api_object


def load_daily_tweets_to_df(date_str):
    file_name = f'{date_str}_clean-dataset.tsv.gz'
    dataset_url = f"https://github.com/thepanacealab/covid19_twitter/blob/master/dailies/{date_str}/{file_name}?raw=true"
    dest_file = f'/tmp/{file_name}'
    if not os.path.isfile(file_name):
        wget.download(dataset_url, out=dest_file)
    df = pd.read_csv(dest_file, sep='\t')
    df = df.set_index('tweet_id')
    return df


def filter_by_language(data_frame, langauage='en'):
    return data_frame[data_frame['lang'] == langauage]


def fetch_tweets(twitter_api_obj, id_df, start, step_size, upto=None):
    end = start + step_size
    ids = list(id_df.index)
    limit = upto if upto else len(ids)
    delayed_retry = 1

    while start < limit:
        print('currently getting {} - {}'.format(start, end))
        sleep(6)  # needed to prevent hitting API rate limit
        id_batch = ids[start:end]

        try:
            tweets = twitter_api_obj.lookup_statuses(id_batch)
            start += step_size
            end += step_size

            yield tweets
        except tweepy.TweepyException as ex:
            print('Caught the TweepError exception:\n %s' % ex)
            sleep(
                30 * delayed_retry
            )  # sleep a bit to see if connection Error is resolved before retrying
            delayed_retry += 1  # increase backoff
            if delayed_retry > 3:
                start += step_size
                end += step_size

            continue


def scrape_tweet_data(date_to_fetch,
                      spark,
                      saver_func,
                      total_tweets_to_fetch=10):
    dailies_df = load_daily_tweets_to_df(date_to_fetch)
    dailies_df_filtered = filter_by_language(dailies_df)
    fetched_tweets_itr = fetch_tweets(build_twitter_api_object(),
                                      dailies_df_filtered, 0, 100,
                                      total_tweets_to_fetch)
    for fetched_tweets in fetched_tweets_itr:
        filtered_tweets = build_filtered_json_list(fetched_tweets)
        if saver_func:
            saver_func(df_spark=spark.createDataFrame(filtered_tweets))
